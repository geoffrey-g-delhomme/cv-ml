{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in: [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.**\n",
    "\n",
    "This module allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.\n",
    "\n",
    "**Use of spatial transformers allows modules to learn invariance to translation, scale, rotation and more generic warping.**\n",
    "\n",
    "![](../../assets/spatial-transformer-networks-1.png)\n",
    "\n",
    "### Localisation Network\n",
    "\n",
    "It takes the input feature map and outputs the parameters of the transformation to be applied to the feature map.\n",
    "\n",
    "**NOTE**: Parameters of the transformation can also be used in trailing layers!\n",
    "\n",
    "### Parameterised Sampling Grid\n",
    "\n",
    "The sampling grid consists in defining how the values from the input feature map ($s$ in $U$) are gathered in the output feature map ($t$ in $V$).\n",
    "\n",
    "![](../../assets/spatial-transformer-networks-2.png)\n",
    "\n",
    "If the transformation $\\mathcal{T}_{\\theta}$ is a 2D affine transformation, it is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_i^s \\\\\n",
    "y_i^s\n",
    "\\end{pmatrix} = \\mathcal{T}_{\\theta}(G_i) = \n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n",
    "\\theta_{21} & \\theta_{22} & \\theta_{23}\n",
    "\\end{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_i^t \\\\\n",
    "y_i^t \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with $\\left(x_i^s, y_i^s\\right) \\in [-1,1]$ and $\\left(x_i^t, y_i^t\\right) \\in [-1, 1]$.\n",
    "\n",
    "Another class of transformation can be the attention:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n",
    "\\theta_{21} & \\theta_{22} & \\theta_{23}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "s & 0 & t_x \\\\\n",
    "0 & s & t_y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "allowing cropping, translation and isotropic scaling by varying $s$, $t_x$ and $t_y$.\n",
    "\n",
    "Other types of transformation functions are also possible:\n",
    "- Projective transformation\n",
    "- 16-point thin plate split transformation ([REF](https://khanhha.github.io/posts/Thin-Plate-Splines-Warping/))\n",
    "  - **Well adapted to reduce error for elastical deformations**\n",
    "\n",
    "### Differentiable Image Sampling\n",
    "\n",
    "To perform a spatial transformation of the input feature map, a sampler must take the set of sampling points $\\mathcal{T}_\\theta(G_i)$, along with the input feature map $U$ and produce the sampled output feature map $V$:\n",
    "\n",
    "$$\n",
    "V_i^c = \\sum_n^H \\sum_m^W U_{nm}^c k\\left(x_i^s-m; \\Phi_x\\right)k\\left(y_i^s-n; \\Phi_n\\right) \\quad \\forall i \\in [1 \\dots H'W'] \\quad \\forall c \\in [1 \\dots C]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\Phi_x$ and $\\Phi_y$ are the parameters of a generic sampling kernel $k()$ which defined the input feature map (i.e. image) interpolation\n",
    "- $U_{nm}^c$ is the value at location $(n, m)$ in channel $c$\n",
    "- $V_i^c$ is the output value for pixel $i$ at location $\\left(x_i^t, y_i^t\\right)$ in channel $c$\n",
    "\n",
    "**NOTE**: The sampling is similar for each channel.\n",
    "\n",
    "### Integer sampling\n",
    "\n",
    "This sampling kernel equates to just copying the value at the nearest pixel in the input feature map.\n",
    "\n",
    "$$\n",
    "V_i^c = \\sum_n^H \\sum_m^W U_{nm}^c \\delta \\left( \\left\\lfloor x_i^s + 0.5 \\right\\rfloor - m \\right) \\delta \\left( \\left\\lfloor y_i^s + 0.5 \\right\\rfloor - m \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\left\\lfloor x_i^s + 0.5 \\right\\rfloor$ rounds $x$ to the nearest integer\n",
    "- $\\delta()$ is the Kronecker delta function (output 1 if $x == 0$ 0 otherwise)\n",
    "\n",
    "### Bilinear sampling\n",
    "\n",
    "This sampling kernel get the weighted sum of values at surrounding pixels in the input feature map.\n",
    "\n",
    "$$\n",
    "V_i^c = \\sum_n^H \\sum_m^W U_{nm}^c \\max(0, 1 - \\left \\vert x_i^s - m \\right \\vert ) \\max(0, 1 - \\left \\vert y_i^s - m \\right \\vert )\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
